b <- samples %>% spread(Missing, p_mean) %>%
mutate(mean_mix = apply(.[,3:ncol(.)], 1, function(pmeans){
if (sum(!is.na(pmeans)) <= 1){
return(NA)
}   else {
sum(pmeans, na.rm = T)
}
})) %>%
group_by(Year) %>%
nest() %>%
mutate(
cdf = map(data, function(d) {
if (is.na(d$mean_mix[1])) return(NA)
ecdf(d$mean_mix)
}),
lower = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.025)
}),
upper = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.975)
}),
lower2 = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.05)
}),
upper2 = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.95)
})) %>%
unnest(upper, lower) %>%
select(Year, upper, lower, upper2, lower2) %>%
full_join(mix_mean)
})
predicted <- map_df(fits$fits, function(fit){
imputed <- fit$fit$summary.fitted.values[, c("mean", "0.025quant", "0.975quant")] %>%
cbind(fits$dat$data[,c("Missing",  "Year")]) %>%
filter(imputed_idx)
mix <- imputed %>%
full_join(missing_weights, by = c("Missing")) %>%
mutate(p_mean = p_sites * mean
)  %>% select(Year, Missing, p_mean)
mix_mean <- mix %>%
right_join(years_to_include, by = c("Year", "Missing")) %>%
spread(Missing, p_mean) %>%
mutate(
mean = apply(.[,2:ncol(.)], 1, function(pmeans){
if (sum(!is.na(pmeans)) <= 1){
return(NA)
}   else {
sum(pmeans, na.rm = T)
}
}),
model = fit$model
)
a <- inla.posterior.sample(result = fit$fit, n = 1000)
samples <- map_df(a, function(a){
fits$dat$data[,c("Missing", "Year")] %>%
cbind(prev = exp(a$latent[1:nrow(.)])/(exp(a$latent[1:nrow(.)]) + 1)) %>%
filter(imputed_idx)
}) %>%
full_join(missing_weights, by = "Missing") %>%
mutate(
sampleno = rep(1:1000, each = sum(imputed_idx)),
sample_prev = map2_dbl(mean_size, prev, function(size, prev) {
rbinom(1, size, prev)/size
}),
p_mean = p_sites * sample_prev
) %>%
select(-prev, -n_sites, -p_sites, - sample_prev, -mean_size) %>%
right_join(years_to_include, by = c("Year", "Missing"))
b <- samples %>% spread(Missing, p_mean) %>%
mutate(mean_mix = apply(.[,3:ncol(.)], 1, function(pmeans){
if (sum(!is.na(pmeans)) <= 1){
return(NA)
}   else {
sum(pmeans, na.rm = T)
}
})) %>%
group_by(Year) %>%
nest() %>%
mutate(
cdf = map(data, function(d) {
if (is.na(d$mean_mix[1])) return(NA)
ecdf(d$mean_mix)
}),
lower = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.025)
}),
upper = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.975)
}),
lower2 = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.05)
}),
upper2 = map_dbl(cdf, function(cdfnum){
if (is.na(cdfnum)) return(NA)
quantile(cdfnum, probs = 0.95)
})) %>%
unnest(upper, lower) %>%
select(Year, upper, lower, upper2, lower2) %>%
full_join(mix_mean)
})
estimates <- map_df(fits$fits, function(f){
f$fit$summary.fitted.values[, c("mean", "0.025quant", "0.975quant")] %>%
cbind(fits$dat$data[,c("Missing",  "Year")]) %>%
filter(imputed_idx) %>%
select(Missing, Year, mean) %>%
mutate(model = f$model)
}) %>%
right_join(years_to_include, by = c("Year", "Missing"))
p <- ggplot(fits$dat$data, aes(x = Year)) +
geom_point(aes(y = prev, color = Missing), alpha = 0.5) +
geom_line(data = estimates, aes(y = mean, color = Missing)) +
geom_line(data = predicted, aes(y = mean)) +
facet_wrap(~model) +
geom_ribbon(data = predicted, aes(ymin = lower, ymax = upper),
alpha = 0.2) +
geom_ribbon(data = predicted, aes(ymin = lower2, ymax = upper2),
alpha = 0.2) +
theme_minimal() +
labs(title = "Prediction for new site")
p
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
library(tidyverse)
metadata <- read_csv("../Data/RC_2017-02_metadata.csv",
col_types = "cccciiciiiiic")
library(tidyverse)
metadata <- read_csv("../Data/RC_2017-02_metadata.csv",
col_types = "cccciiciiiiic")
head(metadata)
library(tidyverse)
metadata <- read_csv("../Data/RC_2017-02_features.csv")
knitr::kable(features[1:10,1:20]
knitr::kable(features[1:10,1:20]
knitr::kable(features[1:10,1:20]
)
?kableExtra
install.packages("kableExtra")
?scroll_box
library(kableExtra)
?scroll_box
library(lme4)
?intervals
install.packages(c("backports", "boot", "crayon", "curl", "data.table", "dplyr", "ggrepel", "Matrix", "MCMCglmm", "mgcv", "openNLPdata", "openssl", "psych", "purrr", "Rcpp", "reticulate", "rJava", "rstudioapi", "textshape", "tidyr", "tidyselect", "tidytext"))
file <- "RC_2017-02"
metadata <- read_csv(paste0(file, "_metadata.csv"))
library(tidyverse)
metadata <- read_csv(paste0(file, "_metadata.csv"))
file <- "../Data/RC_2017-02"
metadata <- read_csv(paste0(file, "_metadata.csv"))
setwd("~/Box Sync/Fall 2017/SoDA 502/502_Project/Code")
metadata <- read_csv(paste0(file, "_metadata.csv"))
install.packages("data.table")
library(data.table)
?fread
wordlen <- fread(paste0(file, "_features.csv"), sep = ",",
select = c("id", "length_words"), stringsAsFactors = F)
rows <- fread(paste0(file, "_features.csv"), sep = ",", nrows = 10)
colnames(rows)
rows[1,]
rows <- fread(paste0(file, "_features.csv"), sep = ",", nrows = 10, header = T)
colnames(rows)
wordlen <- fread(paste0(file, "_features.csv"), sep = ",", header = T,
select = c("id", "length_words"), stringsAsFactors = F)
metadata <- metadata %>% full_join(wordlen, by = c("id"))
unique(metadata$subreddit)
View(metadata %>% filter(is.na(subreddit)))
?setdiff
#filter authors who
# - are not in the test set
# - have word lengths that are too consistent
set1_authors <- intersect(unique(metadata$author[metadata$subreddit == set1[1]]),
unique(metadata$author[metadata$subreddit == set1[2]]))
set1 <- c("AskReddit", "The_Donald")
set2 <- c("worldnews", "news")
#filter authors who
# - are not in the test set
# - have word lengths that are too consistent
set1_authors <- intersect(unique(metadata$author[metadata$subreddit == set1[1]]),
unique(metadata$author[metadata$subreddit == set1[2]]))
set2_authors <- intersect(unique(metadata$author[metadata$subreddit == set2[1]]),
unique(metadata$author[metadata$subreddit == set2[2]]))
set1_posts <- metadata %>% filter(subreddit %in% set1,
author %in% set1_authors)
?quantile
set1_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %>% View
set2_posts <- metadata %>% filter(subreddit %in% set2,
author %in% set2_authors)
set1_delete <- set1_posts %>% filter(subreddit %in% set1[1]) %>%
group_by(author) %>% summarise(n_posts = n())
View(set1_delete)
table(set1_delete)
table(set1_delete$n_posts)
set1_delete <- set1_posts %>% filter(subreddit %in% set1[1]) %>%
group_by(author) %>% summarise(n_posts = n(),
n_words = sum(length_words))
table(set1_delete %>% select(-author))
table(set1_delete$n_posts)
table(set1_delete$length_words)
table(set1_delete$n_words)
set1_delete <- set1_delete  %>%
filter(n_posts <= 5 | n_words <= 500)
set1_posts <- set1_posts %>% filter(!(author %in% set1_delete$author))
length(unique(set1_posts$author))
nrow(set1_posts)
set1_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %>% View
set1_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05),
n_posts = n()) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %>% View
delete_again <- set1_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05),
n_posts = n()) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
filter(diff < 6)
delete_again
length(unique(set1_authors$author))
length(set1_authors)
nrow(set1_delete)
length(unique(set1_posts$author))
colnames(set1_posts)
unique(set1_posts$subreddit)
write_csv(set1_posts %>% select(id, author, subreddit),
"../Data/comments_AskReddit-TheDonald.csv")
# worldnews -> news
set2_delete <- set2_posts %>% filter(subreddit %in% set2[1]) %>%
group_by(author) %>% summarise(n_posts = n(),
n_words = sum(length_words))
table(set2_delete$n_posts)
table(set2_delete$n_words)
set2_delete <- set2_delete  %>%
filter(n_posts <= 5 | n_words <= 500)
set2_posts <- set2_posts %>% filter(!(author %in% set2_delete$author))
length(unique(set2_posts$author))
delete_again <- set2_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05),
n_posts = n()) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
filter(diff < 6)
length(set2_authors)
?sort
table(set1_delete$n_posts) %>% length
table(set1_delete$n_posts)[40:60]
# AskReddit -> The_Donald
set1_delete <- set1_posts %>% filter(subreddit %in% set1[1]) %>%
group_by(author) %>% summarise(n_posts = n(),
n_words = sum(length_words))
View(set1_delete)
set2_delete <- set2_posts %>% filter(subreddit %in% set2[1]) %>%
group_by(author) %>% summarise(n_posts = n(),
n_words = sum(length_words))
table(set2_delete$n_posts)
table(set2_delete$n_words)
set2_delete <- set2_delete  %>%
filter(n_posts <= 5 | n_words <= 500)
set2_posts <- set2_posts %>%
filter(!(author %in% c("AutoModerator", set2_delete$author)))
length(unique(set2_posts$author))
"AutoModerator" %in% set2_posts$author
delete_again <- set2_posts %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05),
n_posts = n()) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
filter(diff < 6)
set2_posts <- set2_posts %>%
filter(!(author %in% delete_again$author))
write_csv(set2_posts %>% select(id, author, subreddit),
"../Data/comments_worldnews-news.csv")
metadata <- read_csv(paste0(file, "_metadata.csv")) %>% #get metadata
#join with word length feature
full_join(fread(paste0(file, "_features.csv"), sep = ",", header = T,
select = c("id", "length_words"), stringsAsFactors = F),
by = c("id")) %>%
filter(!is.na(subreddit))
set1 <- c("AskReddit", "The_Donald")
set2 <- c("worldnews", "news")
file <- "../Data/RC_2017-02"
metadata <- read_csv(paste0(file, "_metadata.csv")) %>% #get metadata
#join with word length feature
full_join(fread(paste0(file, "_features.csv"), sep = ",", header = T,
select = c("id", "length_words"), stringsAsFactors = F),
by = c("id")) %>%
filter(!is.na(subreddit))
metadata <- metadata %>% select(author, length_words) %>%
filter(author != "AutoModerator")
metadata <- read_csv(paste0(file, "_metadata.csv")) %>% #get metadata
#join with word length feature
full_join(fread(paste0(file, "_features.csv"), sep = ",", header = T,
select = c("id", "length_words"), stringsAsFactors = F),
by = c("id")) %>%
filter(!is.na(subreddit))
metadata <- metadata %>% select(id, author, length_words) %>%
filter(author != "AutoModerator")
#word length quantiles
wq <- quantile(metadata$length_words,
probs = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92,
0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995,
0.997, 1))
wq
total_words <- metadata %>% group_by(author) %>%
summarise(n_words = sum(length_words)) %>%
pull(n_words)
#word length quantiles
wq <- quantile(total_words,
probs = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92,
0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995,
0.997, 1))
wq
length(total_words)
100/8
total_words <- metadata %>% group_by(author) %>%
summarise(n_words = sum(length_words))
#remove authors with
remove_author <- total_words %>% filter(n_words > 500) %>% pull(author)
metadata <- metadata %>%
filter(!(author %in% remove_author))
nrow(metadata)
#remove authors with total words <= 500
remove_author <- total_words %>%
filter(n_words > 500,
n_words < 9580) %>% #99.7 quantile
pull(author)
metadata <- metadata %>%
filter(!(author %in% remove_author))
total_words %>%
filter(n_words > 500,
n_words < 9580) %>% View
#now filter based on word diff
delete_again <- metadata %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05),
n_posts = n()) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
filter(diff < 6)
total_words <- metadata %>% group_by(author) %>%
summarise(n_words = sum(length_words),
n_posts = n())
#remove authors with total words <= 500
remove_author <- total_words %>%
filter(n_words > 500,
n_words < 9580,
n_posts < 6) %>% #99.7 quantile
pull(author)
metadata <- metadata %>%
filter(!(author %in% remove_author))
#now filter based on word diff
delete_again <- metadata %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
filter(diff < 6)
nrow(delete_again)
length(unique(metadata$author))
metadata <- read_csv(paste0(file, "_metadata.csv")) %>% #get metadata
#join with word length feature
full_join(fread(paste0(file, "_features.csv"), sep = ",", header = T,
select = c("id", "length_words"), stringsAsFactors = F),
by = c("id")) %>%
filter(!is.na(subreddit))
metadata <- metadata %>% select(id, author, length_words) %>%
filter(author != "AutoModerator")
total_words <- metadata %>% group_by(author) %>%
summarise(n_words = sum(length_words),
n_posts = n())
#word length quantiles
wq <- quantile(total_words$n_words,
probs = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92,
0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995,
0.997, 1))
#remove authors with total words <= 500
remove_author <- total_words %>%
filter(n_words > 500,
n_words < 9580,
n_posts < 6) %>% #99.7 quantile
pull(author)
filtered <- metadata %>%
filter(!(author %in% remove_author))
remove_author <- total_words %>%
filter(n_words < 500 |
n_words > 9580 |
n_posts < 6) %>% #99.7 quantile
pull(author)
filtered <- metadata %>%
filter(!(author %in% remove_author))
#now filter based on word diff
delete_again <- metadata %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View
library(magrittr)
#now filter based on word diff
delete_again <- metadata %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View
#now filter based on word diff
delete_again <- filtered %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View
#now filter based on word diff
delete_again <- filtered %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
write_csv("../Output/word_diff.csv")
delete_again %>% write_csv("../Output/word_diff.csv")
delete_again <- delete_again %>%
filter(diff > 5)
filtered <- filtered %>%
filter(!(author %in% delete_again$author))
#now slice across word length
length(unique(filtered$author))
View(filtered)
total_words <- metadata %>% group_by(author) %>%
summarise(n_words = sum(length_words),
n_posts = n())
#word length quantiles
wq <- quantile(total_words$n_words,
probs = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92,
0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995,
0.997, 1))
#remove authors with total words <= 500
remove_author <- total_words %>%
filter(n_words < 500 |
n_words > 9580 |
n_posts < 6) %>% #99.7 quantile
pull(author)
nrow(total_words)
length(remove_author)
nrow(total_words) - length(remove_author)
filtered <- metadata %>%
filter(!(author %in% remove_author))
#now filter based on word diff
delete_again <- filtered %>% group_by(author) %>%
summarise(word95 = quantile(length_words, probs = 0.95),
word05 = quantile(length_words, probs = 0.05)) %>%
mutate(diff = word95 - word05) %>%
arrange(diff) %T>% View %>%
write_csv("../Output/word_diff.csv")
delete_again <- read_csv("../Output/word_diff.csv")
delete_again <- delete_again %>%
filter(diff < 5)
delete_again <- delete_again %>%
filter(diff < 6)
filtered <- filtered %>%
filter(!(author %in% delete_again$author))
length(unique(filtered$author))
total_words <- filtered %>% group_by(author) %>%
summarise(n_words = sum(length_words),
n_posts = n())
#word length quantiles
wq <- quantile(total_words$n_words,
probs = c(0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.91, 0.92,
0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 0.995,
0.997, 1))
wq
total_words %>% filter(n_words >= 600, n_words <= 700) %>%
sample_n(5000)
#randomly select 5000 authors with total words between 600 and 700
total_words %>% filter(n_words >= 600, n_words <= 700) %>%
sample_n(5000) %>%
write_csv("../Data/500-600words_5000authors.csv")
#train on 1000-1500 words?
total_words %>% filter(n_words >= 1200, n_words <= 1700) %>%
sample_n(5000) %>%
write_csv("../Data/1000-1500words_5000authors.csv")
total_words %>% filter(n_words >= 4000, n_words <= 5000) %>%
sample_n(5000) %>%
write_csv("../Data/4000-5000words_5000authors.csv")
total_words %>% filter(n_words >= 4000, n_words <= 8000) %>%
sample_n(5000) %>%
write_csv("../Data/4000-5000words_5000authors.csv")
total_words %>% filter(n_words >= 4000, n_words <= 8000) %>%
sample_n(5000) %>%
write_csv("../Data/4000-8000words_5000authors.csv")
#slice across authors
total_words %>%
sample_n(50) %>%
write_csv("../Data/50authors.csv")
total_words %>%
sample_n(5000) %>%
write_csv("../Data/5000authors.csv")
total_words %>%
write_csv("../Data/all_authors.csv")
